---
layout: post
title: 读书笔记 数学之美 第四章 谈谈分词
category: 读书
author: 沈伯伟
tags: 数学之美
---

**keyword**：`分词 统计语言模型`

这章主要的核心是`分词`，理论推导并不多，较容易理解。下面简单进行总结。

## 中文分词方法的演变

### 分词的目的

语言模型是建立在词的基础上的，词是表达语义的最小单位。对于西方拼音来说，词之间有分隔符（空格），但是对于中文等语言，词之间并没有分割符，所以需要进行处理。

### 分词的方法

```
中国航天官员应邀到美国与太空总署官员开会 → 中国/航天/官员/应邀/到/美国/与/太空/总署/官员/开会
```

- 最简单有效的方法：`查字典`

把句子从左到右扫描一遍，遇到`字典`里有的词就标识出来，直到找到最长匹配。遇到不能查询到的字串就分割为单字词。
这个方法虽然简单，但是非常有效。不足就是不能很好的解决`二义性`：

```
发展中国家
正确：→ 发展/中/国家
错误：→ 发展/中国/家

北京大学生
正确：→ 北京/大学生
错误：→ 北京大学/生
```

- `词典` + `统计语言模型`

为了解决上边的`二义性`问题，有学者提出了通过`统计语言模型`进行分词的方法（[统计语言模型](https://shenbowei.github.io/2016/10/22/beauty-of-math-chapter3.html "跳转")）。
假设一个句子$S$可以有如下三种不同的分词结果：

$$
A_1,A_2,A_3 \cdots ,A_k\\
B_1,B_2,B_3 \cdots ,B_m\\
C_1,C_2,C_3 \cdots ,C_n
$$

其中 $A_1,A_2,\cdots,B_1,B_2,\cdots,C_1,C_2$ 均为汉语的词，由于不同分词结果可能产生不同的数量的词串，所以分别用$k,m,n$表示分词的数目。
那么最好的分词应该保证分词后出现这个句子的概率最大，假设$A_1,A_2,A_3 \cdots ,A_k$最好，即：

$$
P(A_1,A_2,A_3 \cdots ,A_k) > P(B_1,B_2,B_3 \cdots ,B_m) \quad \&\&\\
P(A_1,A_2,A_3 \cdots ,A_k) > P(C_1,C_2,C_3 \cdots ,C_n)
$$

**实现技巧：**使用动态规划（Dynamic Programming），并利用维特比（Viterbi）算法快速找到最佳分词。

`统计语言模型`的分词方法同样适用于英文词组的分割，或者手写体的西方语言分割（因为手写体的空格并不好判断）。


## 如何衡量分词结果

### 分词的一致性

并不能简单地通过比较计算机分词结果和人工分词结果来衡量分词的好坏。因为人工分词也具有差异性，并不能完全作为衡量标准，只具备参考价值。

### 词的颗粒度和层次

- 颗粒度

人工分词的不一致性主要是人们对词的`颗粒度`的认识不同。在汉语里，词是表达意思的最基本单位，再小意思就变了。
例如，`贾里尼克`这个词是不可拆分的，一旦拆分，和原来的人名就没有联系了。但是对于`清华大学`这四个字，既可以认为是一个整体，即基本词。
又可以认为是一个词组，可以继续拆分为`清华`和`大学`，`清华`是修饰`大学`的定语。

- 不同颗粒度的适用

在不同应用中，会有一种颗粒度好于另一种颗粒度。

```
机器翻译：颗粒度大好
例如：联想公司
→ 大颗粒度 → 联想公司  → lenovo 翻译正确
→ 小颗粒度 → 联想/公司 → associate company 翻译错误
		 
网页搜索：颗粒度小好
例如：清华大学
→ 大颗粒度 → 清华大学  → 用户查询"清华" → 搜索失败
→ 小颗粒度 → 清华/大学 → 用户查询"清华" → 搜索成功
```

### 层次划分

为了针对不同的应用，我们需要让分词器同时支持不同层次的分词。
简单实现如下：

1. 需要一个基本词表和一个复合词表，基本词表包含"清华","大学","贾里尼克"这样不可划分的词，即对应小粒度。
复合词表包含复合词及它们由哪些基本词组成，如"清华大学：清华 - 大学","搜索引擎：搜索 - 引擎"。

2. 根据基本词表和复合词表分别建立语言模型：$L_1$和$L_2$。

3. 根据基本词表和语言模型$L_1$对句子进行分词，得到小粒度的分词结果。输入是字串，输出是基本词串。

4. 再用复合词表和语言模型$L_2$进行第二次分词，得到大粒度的分词结果。输入是基本词串，输出为复合词串。






