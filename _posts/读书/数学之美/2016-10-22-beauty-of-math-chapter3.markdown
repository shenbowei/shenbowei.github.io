---
layout: post
title: 读书笔记 数学之美 第三章 统计语言模型
category: 读书
author: 沈伯伟
tags: 数学之美
---

**keyword**：`统计 概率 马尔可夫`

> 计算机处理自然语言，一个基本问题就是为自然语言这种上下文相关的特性建立数学模型。
> 这个数学模型就是在自然语言处理中常说的统计语言模型。

## 数学的方法描述语言规律

统计语言模型产生的初衷是为了解决语音识别问题。
在语言识别中，计算机需要知道一个文字序列是否能构成一个大家理解并且有意义的语句。
`基于规则的方法`在解决这个问题的时候，试图判断这个文字序列是否合乎文法、含义是否正确。
`基于统计的方法`则只是“简单”地通过`可能性`大小来判断这个文字序列是否合法。

假设$S$表示一个文字序列，由一连串顺序排序的单词组成：$S = \underbrace{w_1,w_2,...,w_n}_{n个}$。
我们的**目标**是想知道$S$在文本中出现的可能性，即数学上的概率$P(S)$。已知：

$$
P(S) = P(w_1, w_2, ... , w_n)
$$

利用条件概率展开：

$$
P(w_1, w_2, ... , w_n) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3| w_1,w_2) \cdots P(w_n|w_1,w_2,...,w_{n-1})
$$

上式右项中，越往后的条件概率越难计算。为了简化计算，使用马尔可夫假设进行近似估计。

```
马尔可夫假设：假设一个当前状态只与之前的一个状态相关。即任意一个词$w_i$只与它的前一个词$w_{i-1}$相关。
```

因此，问题进一步简化为：

$$
P(S) = P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdots P(w_i|w_{i-1} \cdots P(w_n|w_{n-1}))
$$

上式对应的统计语言模型是二元模型（Bigram Model）。类比得到：假设一个词由前边的$N-1$个词决定，那么就是N元模型。
那么好了，接下来的问题就是计算条件概率$P(w_i|w_{i-1})$。

$$
P(wi|w_{i-1}) = \frac{P(w_{i-1},w_i)}{P(w_{i-1})}
$$

即条件概率进一步如上展开为联合概率$P(w_{i-1},w_i)$和边缘概率$P(w_{i-1})$的形式。

而现在我们拥有的数据包括：大量的机读文本（语料库Corpus）。在这些文本中，我们可以数出$w_{i-1},w_i$这对词相邻出现的次数 #$(w_{i-1},w_i)$ ，
以及$w_{i-1}$本身出现的次数 #$(w_{i-1})$ 。整个语料库的大小为 # 。即可得到频度：

$$
f(w_{i-1},w_i) = \frac{\#(w_{i-1},w_i)}{\#}
\\
f(w_{i-1}) = \frac{\#(w_{i-1})}{\#}
$$

根据`大数定理`，只要统计量足够，相对频度就等于概率，因此：

$$
P(w_{i-1},w_i) \approx \frac{\#(w_{i-1},w_i)}{\#}
\\
P(w_{i-1}) \approx \frac{\#(w_{i-1})}{\#}
$$

因此，我们前面推导的条件概率可以表示为：

$$
P(wi|w_{i-1}) = \frac{\#(w_{i-1},w_i)}{\#(w_{i-1})}
$$

到此，一个复杂的问题被简化为“数一数”的问题。正如书中所说：`数学的精彩之处在于简单的模型可以干大事`。

To be continued...

