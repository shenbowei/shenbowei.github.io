---
layout: post
title: 读书笔记：数学之美-信息的度量和作用
category: 读书
author: shenbowei
tags: 数学之美
---

**keyword**：`信息熵`，`条件熵`，`互信息`，`相对熵`

## 信息熵

`信息熵`是用来解决信息的度量问题，并且量化出信息的作用。
一条信息的信息量和其不确定性有着直接的关系。某种程度而言，信息量就等于不确定性的多少。

```
举例：
32支球队参加的足球赛，结束后让你猜哪只队伍是冠军（你之前没有任何信息），每次告诉你对错，需要猜几次？
很简答，我们二分猜，每次猜一半，只需要log32=5次就能确定冠军是谁了。
```

香农用“比特”（Bit）这个概念来度量信息量，一个比特是一位二进制数，上述例子中“谁是冠军”的信息量是5比特。

```
而现实每只球队的夺冠概率是不一样的，不必平等二分猜，首先在夺冠概率大的球队中猜即可减少猜测次数，即信息量。
```

因此，香农指出，上述例子准确的信息量的定义应该是：

$$
H = -(p_1 \cdot logp_1 + p_2 \cdot logp_2 + \cdots + p_32 \cdot logp_32)
$$

香农把它定义为`信息熵`，一般用H表示，单位是比特。对于任意一个随机变量X，它的熵定义为：

$$
H(X) = - \sum_{x \in X} P(x)logP(x)
$$

变量的不确定性越大，熵也越大，要把它搞清楚，需要的信息量也就越大。

## 条件熵

信息是消除系统不确定性的唯一方法。一个事物的内部会存在随机性，即不确定性，假定为$U$。
而从外部消除这个不确定性的唯一方法就是引入信息$I$，而需要引入的信息量取决于这个不确定性的大小，即$I>U$才行。
当$I<U$时，这些信息可以消除一部分不确定性，新的不确定为：

$$
U' = U-I
$$

知道的信息越多，随机事件的不确定性越小。通过获取这些相关信息，可以帮助我们来更加了解对象，从而降低不确定性。
如自然语言模型中，一元模型就是通过某个词本身的概率分布来消除不确定因素；而二元及更高阶的语言模型则还使用了上下文的信息。
在数学上可以严格地证明为什么这些相关信息能够消除不确定性。为此。引入了`条件熵`的概念。

假定由X和Y两个随机变量，X是我们需要了解的。假设我们知道X的随机分布P(X)，那么便知道了X的熵为：

$$
H(X) = - \sum_{x \in X} P(x)logP(x)
$$

现在假设我们还知道了Y的信息，包括它和X一起出现的概率，数学上称为联合概率分布（Joint Probability）。
以及在Y取不同值的前提下X的概率分布，数学上称为条件概率分布（Conditional Probability）。
定义在Y的条件下的`条件熵`为：

$$
H(X|Y) = - \sum_{x \in X,y \in Y} P(x,y)logP(x|y)
$$

可证明$H(X) \ge H(X\|Y)$，即多了Y的信息，X的不确定性下降了。
也说明了为什么自然语言模型中二元模型的不确定性小于一元模型。
同理，两个条件的`条件熵`为：

$$
H(X|Y,Z) = - \sum_{x \in X,y \in Y, z \in Z} P(x,y,z)logP(x|y,z)
$$

同样可以证明$H(X\|Y) \ge H(X\|Y,Z)$。即三元模型要比二元好。

总结：信息的作用就是在于消除不确定性，自然语言处理的大量问题就是寻找相关的信息。

## 互信息

相关信息可以帮助我们来消除不确定性，那么如何描述两个信息之间是否相关呢？
香农在信息论中提出了`互信息`（Mutual Information）的概念作为两个随机事件“相关性”的量化度量。

假设有两个随机事件X和Y，它们的`互信息`定义如下：

$$
I(x;y) = - \sum_{x \in X,y \in Y} P(x,y)log \frac{P(x,y)}{P(x) \cdot P(y)}
$$

可以证明这个互信息就是随机事件X的信息熵$H(X)$和已知随机事件Y条件下的条件熵$H(X\|Y)$之间的差，即：

$$
I(x;y) = H(X) - H(X|Y)
$$

推导过程就是利用了log函数的和差性质,可参考：[互信息](http://baike.baidu.com/item/互信息 "跳转")。

所谓两个事件的相关性的量化度量，就是在了解了其中一个Y的前提下，对消除另一个X不确定性所提供的的信息量。
互信息是一个取值在0到$min(H(X),H(Y))$之间的函数。当X和Y完全相关时，$I(x;y) = H(X)$（**注意：这里书中说取值为1，应该是有问题的**）；
当X和Y完全无关时，$I(x;y) = 0$。

```
互信息的应用：
解决词义的二义性：Bush=美国总统布什=灌木丛；

解决办法：
从大量文本中找出和总统布什一起出现的互信息最大的一组词（例如总统，美国，国会，华盛顿等）。
再同样找出和灌木丛一起出现的互信息最多的词（例如土壤，植物，野生等）。
有了这两组词，在翻译Bush的时候，只需看其上下文中哪类相关的词多就可以了。

这种方法由吉尔（William Gale）、丘吉（Kenneth Church）和雅让斯基（David Yarowsky）提出。
```

## 相对熵

`相对熵`，也称为`交叉熵`，英文为Kullback-Leibler Divergence，是以它的两个提出者库尔贝克和莱伯勒的名字命名的，也用来衡量相关性。
但是和变量的互信息不同，`相对熵`用来衡量两个取值为正数的函数的相似性，定义为：

$$
KL(f(x)\|g(x)) = \sum_{x \in X} f(x) \cdot log \frac{f(x)}{g(x)}
$$

需要记住下面的三条结论：

1. 对于两个完全相同的函数，它们的`相对熵`等于零。

2. `相对熵`越大，两个函数的差异越大；反之，`相对熵`越小，两个函数的差异越小。

3. 对于概率分布函数或者概率密度函数，如果取值均大于零，`相对熵`可以度量两个随机分布的差异性。

`相对熵`并不是对称的：

$$
KL(f(x)\|g(x)) \ne KL(g(x)\|f(x))
$$

这样使用起来有时不是很方便，为了让它对称，詹森和香农提出了新的`相对熵`计算方法，将上述不等式两边取均值：

$$
JS(f(x)\|g(x)) = \frac{1}{2}[KL(f(x)\|g(x)) + KL(g(x)\|f(x))]
$$
